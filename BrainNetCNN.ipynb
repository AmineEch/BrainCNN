{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BrainNetCNN \n",
    "version 1 <br>\n",
    "Software by **Amine Echraibi**<br>\n",
    "This notebook implements the method described [Here](http://www.sciencedirect.com/science/article/pii/S1053811916305237) and verifies the results. <br>\n",
    "This is a re-implementation in Keras Tensorflow backend, of the neural net described in the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "plt.interactive(False)\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "from scipy.stats import pearsonr\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import optimizers, callbacks, regularizers, initializers\n",
    "from E2E_conv import *\n",
    "from injury import ConnectomeInjury\n",
    "import numpy as np\n",
    "from vis.visualization import visualize_activation\n",
    "from keras.utils import to_categorical\n",
    "from nilearn import datasets\n",
    "from nilearn import input_data\n",
    "from nilearn.connectome import ConnectivityMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Setting up the hyper parameters, and l2 regularizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Globals : Hyperparamaters\n",
    "batch_size = 14\n",
    "dropout = 0.5\n",
    "momentum = 0.9\n",
    "noise_weight = 0.0625\n",
    "lr = 0.01\n",
    "decay = 0.0005\n",
    "\n",
    "# Setting l2_norm regularizer\n",
    "reg = regularizers.l2(decay)\n",
    "kernel_init = initializers.he_uniform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ## Representation of syntheticly generated connectome\n",
    " ### Generating data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading synthetic data\n",
    "injuryconnectome = ConnectomeInjury()\n",
    "x_train,y_train = injuryconnectome.generate_injury(n_samples=1000,noise_weight=noise_weight)\n",
    "x_valid,y_valid = injuryconnectome.generate_injury(n_samples=300,noise_weight=noise_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ploting a synthetic connectome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntfbyN2s2R629/M2azVH7/wGLemoizrFjvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f00c6c63a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The adjacency matrix : \n",
      "[[ 0.04707884  0.03092055  0.14618641 ...,  0.02760327  0.0438556\n",
      "   0.03770889]\n",
      " [ 0.03092055  0.03429337  0.04287777 ...,  0.01300712  0.03443554\n",
      "   0.01435285]\n",
      " [ 0.14618641  0.04287777  0.00859312 ...,  0.05135169  0.03395176\n",
      "   0.01405274]\n",
      " ..., \n",
      " [ 0.02760327  0.01300712  0.05135169 ...,  0.01938141  0.01424499\n",
      "   0.16146842]\n",
      " [ 0.0438556   0.03443554  0.03395176 ...,  0.01424499  0.04410799\n",
      "   0.03664002]\n",
      " [ 0.03770889  0.01435285  0.01405274 ...,  0.16146842  0.03664002\n",
      "   0.01110039]]\n"
     ]
    }
   ],
   "source": [
    "# ploting a sample\n",
    "%matplotlib inline\n",
    "plt.imshow(x_train[0][0])\n",
    "plt.title('synthetic connectome')\n",
    "plt.show()\n",
    "print(\"The adjacency matrix : \")\n",
    "print(x_train[0][0])\n",
    "\n",
    "\n",
    "# reshaping data\n",
    "x_train = x_train.reshape(x_train.shape[0],x_train.shape[3],x_train.shape[2],x_train.shape[1])\n",
    "x_valid = x_valid.reshape(x_valid.shape[0],x_valid.shape[3],x_valid.shape[2],x_valid.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the synthetic connectome is represented as a graph $G(A,\\Omega)$ where : <br> \n",
    "- $\\Omega$ is the set of nodes each node represents a region of the brain.\n",
    "- $ A $ is the weighted adjency matrix of the adges representing the strength of the connections between the regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the new layers\n",
    "### E2E layer \n",
    "$$ A^{l+1,n}_{i,j} = \\sum_{k=1}^{|\\Omega|} \\sum_{m=1}^{M^{l}}[ r_k^{l,m,n}A_{i,k}^{l,m} +   c_k^{l,m,n}A_{k,j}^{l,m}] $$\n",
    "To construct this layer, we must create two kernels corresponding to r and c of size $1x|\\Omega|$ and $|\\Omega|x1$ respectively, we then convolve with the output filters, duplicate them $|\\Omega|$ times than sum the duplications elementwize to create the convolution discribed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construction of the Edge-2-Edge layer using Keras backend.\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import activations\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.utils import conv_utils\n",
    "\n",
    "class E2E_conv(Layer):\n",
    "    def __init__(self, rank,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=1,\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=1,\n",
    "                 activation=None,\n",
    "                 use_bias=False,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "        super(E2E_conv, self).__init__(**kwargs)\n",
    "        self.rank = rank\n",
    "        self.filters = filters\n",
    "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
    "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
    "        self.padding = conv_utils.normalize_padding(padding)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "        if input_shape[channel_axis] is None:\n",
    "            raise ValueError('The channel dimension of the inputs '\n",
    "                             'should be defined. Found `None`.')\n",
    "        input_dim = input_shape[channel_axis]\n",
    "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
    "\n",
    "        self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(shape=(self.filters,),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        name='bias',\n",
    "                                        regularizer=self.bias_regularizer,\n",
    "                                        constraint=self.bias_constraint)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        # Set input spec.\n",
    "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
    "                                    axes={channel_axis: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        kernel_shape=K.get_value(self.kernel).shape\n",
    "        d=kernel_shape[1]\n",
    "        kernel1xd = K.reshape(self.kernel[0,:],(1,kernel_shape[1],kernel_shape[2],kernel_shape[3]))\n",
    "        kerneldx1 = K.reshape(self.kernel[1,:], (kernel_shape[1],1, kernel_shape[2], kernel_shape[3]))\n",
    "        conv1xd = K.conv2d(\n",
    "            inputs,\n",
    "            kernel1xd,\n",
    "            strides=self.strides,\n",
    "            padding=self.padding,\n",
    "            data_format=self.data_format,\n",
    "            dilation_rate=self.dilation_rate)\n",
    "        convdx1 = K.conv2d(\n",
    "                inputs,\n",
    "                kerneldx1,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate)\n",
    "        concat1 = K.concatenate([convdx1]*d,axis=1)\n",
    "        concat2 = K.concatenate([conv1xd]*d,axis=2)\n",
    "        return concat1+concat2\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0],input_shape[1],input_shape[2],self.filters)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'rank': self.rank,\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'strides': self.strides,\n",
    "            'padding': self.padding,\n",
    "            'data_format': self.data_format,\n",
    "            'dilation_rate': self.dilation_rate,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(E2E_conv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.b.** the previous cell is here to show how the E2E layer was implemented as a keras layer. \n",
    "\n",
    "To use the layer, simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from E2E_conv import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2N layer \n",
    "$$a^{l+1,n}_i = \\sum_{k=1}^{|\\Omega|} \\sum_{m=1}^{M^{l}}[ r_k^{l,m,n}A_{i,k}^{l,m} +   c_k^{l,m,n}A_{k,i}^{l,m}]$$\n",
    "A simple 1-D convolution. Code in model architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2N layer \n",
    "$$ a^{l+1,n} = \\sum_{k=1}^{|\\Omega|} \\sum_{m=1}^{M^{l}} w_k^{l,m,n}a_{i}^{l,m}$$\n",
    "A simple 1-D convolution. Code in model architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "### Description :\n",
    "![alt text](screen1.png \"Description of the different layers\")\n",
    "l2_regularization was introduced in every layer. \n",
    "Dropout of 0.5 was introduced for every layer after N2G layer.\n",
    "The Activation uses very leaky rectified linear units '$\\alpha=0.33$'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model architecture \n",
    "\n",
    "model = Sequential()\n",
    "model.add(E2E_conv(2,32,(2,90),kernel_regularizer=reg,input_shape=(90,90,1),input_dtype='float32',data_format=\"channels_last\"))\n",
    "print(\"First layer output shape :\"+str(model.output_shape))\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "#print(model.output_shape)\n",
    "model.add(E2E_conv(2,32,(2,90),kernel_regularizer=reg,data_format=\"channels_last\"))\n",
    "print(model.output_shape)\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "model.add(Convolution2D(64,(1,90),kernel_regularizer=reg,data_format=\"channels_last\"))\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "model.add(Convolution2D(256,(90,1),kernel_regularizer=reg,data_format=\"channels_last\"))\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "#print(model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128,kernel_regularizer=reg,kernel_initializer=kernel_init))\n",
    "#print(model.output_shape)\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "#print(model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(30,kernel_regularizer=reg,kernel_initializer=kernel_init))\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "#print(model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,kernel_regularizer=reg,kernel_initializer=kernel_init))\n",
    "model.add(Flatten())\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "model.summary()\n",
    "#print(model.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The model\n",
    "- We use the euclidean distance as a cost function\n",
    "- The evaluation metric is the mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from injury import ConnectomeInjury\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from numpy import std\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "def generate_synthetic_validation_data(noise):\n",
    "    injury = ConnectomeInjury() # Generate train/test synthetic data.\n",
    "    x_valid_y_valid = injury.generate_injury(n_samples=300, noise_weight=noise)\n",
    "    return x_valid_y_valid\n",
    "\n",
    "\n",
    "def get_results_from_models(model,noises):\n",
    "    results = [[\"noises\"]+noises,\n",
    "               [\"mae_alpha\"],\n",
    "               [\"stdae_alpha\"],9\n",
    "               [\"mae_beta\"],\n",
    "               [\"stdae_beta\"],\n",
    "               [\"r_alpha\"],\n",
    "               [\"r_beta\"]]\n",
    "    for i in range(len(noises)):\n",
    "        noise = noises[i]\n",
    "        x_valid, y_valid = generate_synthetic_validation_data(noise)\n",
    "        x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[3], x_valid.shape[2], x_valid.shape[1])\n",
    "        # load weights into new model\n",
    "        model.load_weights(\"Weights/BrainCNNWeights_noise_\"+str(noise)+\".h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "        preds = model.predict(x_valid)\n",
    "        results[1].append(\"{0:.2f}\".format(100*mae(preds[:,0],y_valid[:,0])))\n",
    "        results[2].append(\"{0:.2f}\".format(100*std(abs(y_valid[:, 0] - preds[:, 0]))))\n",
    "        results[3].append(\"{0:.2f}\".format(100*mae(preds[:, 1], y_valid[:, 1])))\n",
    "        results[4].append(\"{0:.2f}\".format(100*std(abs(y_valid[:,1]-preds[:,1]))))\n",
    "        results[5].append(\"{0:.2f}\".format(pearsonr(preds[:,0],y_valid[:,0])[0]))\n",
    "        results[6].append(\"{0:.2f}\".format(pearsonr(preds[:, 1], y_valid[:, 1])[0]))\n",
    "    display(HTML(tabulate.tabulate(results, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function\n",
    "can be run in two modes the training mode where we train the model on the genrated data above (set the desired noise weight above), or the predicting mode where we run our generated trained model on different noise weights and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = optimizers.SGD(momentum=momentum,nesterov=True,lr=lr)\n",
    "model.compile(optimizer=opt,loss='mean_squared_error',metrics=['mae'])\n",
    "csv_logger = callbacks.CSVLogger('BrainCNN.log')\n",
    "command = str(raw_input(\"Train or predict ? [t/p] \"))\n",
    "if command == \"t\":\n",
    "    print(\"Training for noise = \"+str(noise_weight))\n",
    "    history=model.fit(x_train,y_train,nb_epoch=1000,verbose=1,callbacks=[csv_logger])\n",
    "    model.save_weights(\"Weights/BrainCNNWeights_noise_\"+str(noise_weight)+\".h5\")\n",
    "else:\n",
    "    print(\"[*] Predicting and printing results for the models trained :\")\n",
    "    get_results_from_models(model,noises = [0,0.0625,0.125,0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the table above describes the results of the mean absolute error, standard deviation absolute error, and correlation for each of the parameters alpha and beta for different levels of noise.\n",
    "#### Conclusion: \n",
    "The table above verifies the results described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Verifying the visualization methode \"Activation-Maximization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating injury signitures S_1 ans S_2\n",
    "\n",
    "def get_symmetric_noise(m, n):\n",
    "    \"\"\"Return a random noise image of size m x n with values between 0 and 1.\"\"\"\n",
    "\n",
    "    # Generate random noise image.\n",
    "    noise_img = np.random.rand(m, n)\n",
    "\n",
    "    # Make the noise image symmetric.\n",
    "    noise_img = noise_img + noise_img.T\n",
    "\n",
    "    # Normalize between 0 and 1.\n",
    "    noise_img = (noise_img - noise_img.min()) / (noise_img.max() - noise_img.min())\n",
    "\n",
    "    assert noise_img.max() == 1  # Make sure is between 0 and 1.\n",
    "    assert noise_img.min() == 0\n",
    "    assert (noise_img.T == noise_img).all()  # Make sure symmetric.\n",
    "\n",
    "    return noise_img\n",
    "\n",
    "def simulate_injury(X, weight_A, sig_A):\n",
    "    denom = (np.ones(X.shape) + (weight_A * sig_A))\n",
    "    X_sig_AB = np.divide(X, denom)\n",
    "    return X_sig_AB\n",
    "\n",
    "def apply_injury_and_noise(X, Sig_A, weight_A,noise_weight):\n",
    "    \"\"\"Returns a symmetric, signed, noisy, adjacency matrix with simulated injury from two sources.\"\"\"\n",
    "    X_sig_AB = simulate_injury(X, weight_A, Sig_A)\n",
    "    # Get the noise image.\n",
    "    noise_img = get_symmetric_noise(X.shape[0], X.shape[1])\n",
    "\n",
    "    # Weight the noise image.\n",
    "    weighted_noise_img = noise_img * noise_weight\n",
    "\n",
    "    # Add the noise to the original image.\n",
    "    X_sig_AB_noise = X_sig_AB + weighted_noise_img\n",
    "    \n",
    "\n",
    "    assert (X_sig_AB_noise[0,:,:].T == X_sig_AB_noise[0,:,:]).all()  # Make sure still is symmetric.\n",
    "\n",
    "    return X_sig_AB_noise\n",
    "\n",
    "\n",
    "def generate_injury_signatures(X_mn, r_state,sig_indexes):\n",
    "        \"\"\"Generates the signatures that represent the underlying signal in our synthetic experiments.\n",
    "\n",
    "        d : (integer) the size of the input matrix (assumes is size dxd)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the strongest regions, which we will apply simulated injuries\n",
    "        sig_indexes = sig_indexes\n",
    "        d = X_mn.shape[0]\n",
    "\n",
    "        S = []\n",
    "\n",
    "        # Create a signature for\n",
    "        for idx, sig_idx in enumerate(sig_indexes):\n",
    "            # Okay, let's make some signature noise vectors.\n",
    "            A_vec = r_state.rand((d))\n",
    "            # B_vec = np.random.random((n))\n",
    "\n",
    "            # Create the signature matrix.\n",
    "            A = np.zeros((d, d))\n",
    "            A[:, sig_idx] = A_vec\n",
    "            A[sig_idx, :] = A_vec\n",
    "            S.append(A)\n",
    "            \n",
    "            assert (A.T == A).all()  # Check if matrix is symmetric.\n",
    "\n",
    "        return np.asarray(S)\n",
    "def sample_injury_strengths(n_samples, X_mn, A, noise_weight):\n",
    "        \"\"\"Returns n_samples connectomes with simulated injury from two sources.\"\"\"\n",
    "        mult_factor = 10\n",
    "\n",
    "        n_classes = 1\n",
    "\n",
    "        # Range of values to predict.\n",
    "        n_start = 0.5\n",
    "        n_end = 1.4\n",
    "        # amt_increase = 0.1\n",
    "\n",
    "        # These will be our Y.\n",
    "        A_weights = np.random.uniform(n_start, n_end, [n_samples])\n",
    "\n",
    "        X_h5 = np.zeros((n_samples, 1, X_mn.shape[0], X_mn.shape[1]), dtype=np.float32)\n",
    "        Y_h5 = np.zeros((n_samples, n_classes), dtype=np.float32)\n",
    "\n",
    "        for idx in range(n_samples):\n",
    "            w_A = A_weights[idx]\n",
    "\n",
    "            # Get the matrix.\n",
    "            X_sig = apply_injury_and_noise(X_mn, A, w_A * mult_factor, noise_weight)\n",
    "\n",
    "            # Normalize.\n",
    "            X_sig = (X_sig - X_sig.min()) / (X_sig.max() - X_sig.min())\n",
    "\n",
    "            # Put in h5 format.\n",
    "            X_h5[idx, 0, :, :] = X_sig\n",
    "            Y_h5[idx, :] = [w_A]\n",
    "\n",
    "        return X_h5, Y_h5\n",
    "    \n",
    "def load_base_connectome():\n",
    "    X_mn = scipy.io.loadmat(\"data/base.mat\")\n",
    "    X_mn = X_mn['X_mn']\n",
    "    return X_mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Adhd data for visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adhd_data = datasets.fetch_adhd(n_subjects=20)\n",
    "msdl_data = datasets.fetch_atlas_msdl()\n",
    "msdl_coords = msdl_data.region_coords\n",
    "\n",
    "masker = input_data.NiftiMapsMasker(\n",
    "    msdl_data.maps, resampling_target=\"data\", t_r=2.5, detrend=True,\n",
    "    low_pass=.1, high_pass=.01, memory='nilearn_cache', memory_level=1)\n",
    "adhd_subjects = []\n",
    "pooled_subjects = []\n",
    "site_names = []\n",
    "adhd_labels = []  # 1 if ADHD, 0 if control\n",
    "for func_file, confound_file, phenotypic in zip(\n",
    "        adhd_data.func, adhd_data.confounds, adhd_data.phenotypic):\n",
    "    time_series = masker.fit_transform(func_file, confounds=confound_file)\n",
    "    pooled_subjects.append(time_series)\n",
    "    is_adhd = phenotypic['adhd']\n",
    "    if is_adhd:\n",
    "        adhd_subjects.append(time_series)\n",
    "\n",
    "    site_names.append(phenotypic['site'])\n",
    "    adhd_labels.append(is_adhd)\n",
    "\n",
    "conn_measure = ConnectivityMeasure(kind=\"tangent\")\n",
    "X = conn_measure.fit_transform(pooled_subjects)\n",
    "Y = np.array(adhd_labels,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this section we will create two sets of data control using a mean connectome for adhd control patients plus noise, and a simulated test with injury using the mean connectome. than we will train brainnetcnn to predict wether a sample is in control or test in order to visualize wether or not it can extract the brain injury pattern that we introduced on the test dataset using keras-vis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_shape : (2, 39, 1, 8)\n",
      "data shape : (?, 39, 39, 1)\n",
      "(1, 39, 1, 8)\n",
      "(39, 1, 1, 8)\n",
      "cat1(?, 39, 39, 8)\n",
      "cat2<dtype: 'float32'>\n",
      "First layer output shape :(None, 39, 39, 8)\n",
      "(None, 39, 39, 8)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "e2e_conv_1 (E2E_conv)        (None, 39, 39, 8)         624       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 39, 39, 8)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 39, 39, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 39, 1, 32)         10016     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 39, 1, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 1, 90)          112410    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 1, 90)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 90)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 1, 64)          5824      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 1, 10)          650       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 1, 1, 10)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 1, 10)          0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1, 1, 2)           22        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 129,546\n",
      "Trainable params: 129,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "batch_size = 14\n",
    "dropout = 0.5\n",
    "momentum = 0.9\n",
    "lr = 0.001\n",
    "decay = 0.0005\n",
    "noise_weight = 0.0625\n",
    "\n",
    "reg = regularizers.l2(decay)\n",
    "kernel_init = initializers.he_uniform()\n",
    "\n",
    "# Model architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(E2E_conv(2,8,(2,39),kernel_regularizer=reg,input_shape=(39,39,1),input_dtype='float32',data_format=\"channels_last\"))\n",
    "print(\"First layer output shape :\"+str(model.output_shape))\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "#print(model.output_shape)\n",
    "print(model.output_shape)\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "model.add(Convolution2D(32,(1,39),kernel_regularizer=reg,data_format=\"channels_last\"))\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "model.add(Convolution2D(90,(39,1),kernel_regularizer=reg,data_format=\"channels_last\"))\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "#print(model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64,kernel_regularizer=reg,kernel_initializer=kernel_init))\n",
    "#print(model.output_shape)\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "#print(model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10,kernel_regularizer=reg,kernel_initializer=kernel_init))\n",
    "model.add(LeakyReLU(alpha=0.33))\n",
    "#print(model.output_shape)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation = \"softmax\",kernel_regularizer=reg,kernel_initializer=kernel_init))\n",
    "model.add(Flatten())\n",
    "model.summary()\n",
    "#print(model.output_shape)\n",
    "\n",
    "\n",
    "opt = optimizers.SGD(nesterov=True,lr=lr)\n",
    "model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "csv_logger = callbacks.CSVLogger('predict_age.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 39, 39)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "control = X[Y==0]\n",
    "print(control.shape)\n",
    "r_state = np.random.RandomState(41)\n",
    "base_connectome = control.mean(axis=0)\n",
    "base_connectome = (base_connectome.T + base_connectome)/2\n",
    "S = generate_injury_signatures(X_mn=base_connectome,r_state=r_state,sig_indexes=[5])\n",
    "X1,Y = sample_injury_strengths(1000,base_connectome,S,noise_weight)\n",
    "S = generate_injury_signatures(X_mn=base_connectome,r_state=r_state,sig_indexes=[30])\n",
    "X2,Y = sample_injury_strengths(1000,base_connectome,S,noise_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1, 39, 39)\n",
      "(2000, 39, 39, 1)\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate([X1,X2],axis=0)\n",
    "print(X.shape)\n",
    "X = X.reshape(X.shape[0],X.shape[2],X.shape[3],1)\n",
    "print(X.shape)\n",
    "Y1 = np.array([1]*1000)\n",
    "Y2 = np.array([0]*1000)\n",
    "Y = np.concatenate([Y1,Y2],axis=0)\n",
    "Y = to_categorical(Y,2)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amine/.local/lib/python2.7/site-packages/keras/models.py:851: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.7998 - acc: 0.5095     \n",
      "Epoch 2/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7990 - acc: 0.5115     \n",
      "Epoch 3/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7972 - acc: 0.5340     \n",
      "Epoch 4/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7974 - acc: 0.5470     \n",
      "Epoch 5/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7964 - acc: 0.5425     \n",
      "Epoch 6/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7940 - acc: 0.5705     \n",
      "Epoch 7/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7917 - acc: 0.5860     \n",
      "Epoch 8/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7906 - acc: 0.5670     \n",
      "Epoch 9/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7905 - acc: 0.5750     \n",
      "Epoch 10/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7887 - acc: 0.5990     \n",
      "Epoch 11/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7852 - acc: 0.6075     \n",
      "Epoch 12/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.7839 - acc: 0.6105     \n",
      "Epoch 13/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7824 - acc: 0.6400     \n",
      "Epoch 14/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7778 - acc: 0.6500     \n",
      "Epoch 15/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7748 - acc: 0.6540     \n",
      "Epoch 16/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7686 - acc: 0.6965     \n",
      "Epoch 17/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7645 - acc: 0.7030     \n",
      "Epoch 18/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7583 - acc: 0.7210     \n",
      "Epoch 19/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7522 - acc: 0.7205     \n",
      "Epoch 20/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7366 - acc: 0.7535     \n",
      "Epoch 21/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7279 - acc: 0.7635     \n",
      "Epoch 22/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.7165 - acc: 0.7885     \n",
      "Epoch 23/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.6945 - acc: 0.8220     \n",
      "Epoch 24/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.6756 - acc: 0.8390     \n",
      "Epoch 25/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.6519 - acc: 0.8480     \n",
      "Epoch 26/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.6274 - acc: 0.8755     \n",
      "Epoch 27/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.5959 - acc: 0.8880     \n",
      "Epoch 28/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.5657 - acc: 0.8950     \n",
      "Epoch 29/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.5361 - acc: 0.9160     \n",
      "Epoch 30/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.4901 - acc: 0.9335     \n",
      "Epoch 31/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.4694 - acc: 0.9200     \n",
      "Epoch 32/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.4174 - acc: 0.9460     \n",
      "Epoch 33/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.4047 - acc: 0.9440     \n",
      "Epoch 34/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.3736 - acc: 0.9460     \n",
      "Epoch 35/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.3480 - acc: 0.9535     \n",
      "Epoch 36/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.3285 - acc: 0.9545     \n",
      "Epoch 37/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.3003 - acc: 0.9610     \n",
      "Epoch 38/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2882 - acc: 0.9635     \n",
      "Epoch 39/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2758 - acc: 0.9700     \n",
      "Epoch 40/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2720 - acc: 0.9635     \n",
      "Epoch 41/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2518 - acc: 0.9680     \n",
      "Epoch 42/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2346 - acc: 0.9765     \n",
      "Epoch 43/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2352 - acc: 0.9715     \n",
      "Epoch 44/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2241 - acc: 0.9785     \n",
      "Epoch 45/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2182 - acc: 0.9750     \n",
      "Epoch 46/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2086 - acc: 0.9850     \n",
      "Epoch 47/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2049 - acc: 0.9785     \n",
      "Epoch 48/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.2015 - acc: 0.9780     \n",
      "Epoch 49/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1926 - acc: 0.9845     \n",
      "Epoch 50/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1915 - acc: 0.9815     \n",
      "Epoch 51/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1800 - acc: 0.9830     \n",
      "Epoch 52/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1784 - acc: 0.9840     \n",
      "Epoch 53/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1814 - acc: 0.9835     \n",
      "Epoch 54/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1752 - acc: 0.9825     \n",
      "Epoch 55/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1766 - acc: 0.9835     \n",
      "Epoch 56/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1742 - acc: 0.9835     \n",
      "Epoch 57/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1682 - acc: 0.9900     \n",
      "Epoch 58/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1689 - acc: 0.9895     \n",
      "Epoch 59/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1656 - acc: 0.9900     \n",
      "Epoch 60/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1649 - acc: 0.9860     \n",
      "Epoch 61/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1686 - acc: 0.9870     \n",
      "Epoch 62/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1589 - acc: 0.9890     \n",
      "Epoch 63/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1653 - acc: 0.9860     \n",
      "Epoch 64/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1551 - acc: 0.9915     \n",
      "Epoch 65/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1598 - acc: 0.9860     \n",
      "Epoch 66/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1604 - acc: 0.9875     \n",
      "Epoch 67/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1584 - acc: 0.9870     \n",
      "Epoch 68/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1526 - acc: 0.9925     \n",
      "Epoch 69/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1546 - acc: 0.9875     \n",
      "Epoch 70/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1516 - acc: 0.9905     \n",
      "Epoch 71/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1473 - acc: 0.9900     \n",
      "Epoch 72/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1511 - acc: 0.9870     \n",
      "Epoch 73/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1523 - acc: 0.9895     \n",
      "Epoch 74/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1469 - acc: 0.9900     \n",
      "Epoch 75/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1454 - acc: 0.9915     \n",
      "Epoch 76/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1498 - acc: 0.9865     \n",
      "Epoch 77/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1494 - acc: 0.9890     \n",
      "Epoch 78/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.1448 - acc: 0.9905     \n",
      "Epoch 79/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1486 - acc: 0.9860     \n",
      "Epoch 80/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1453 - acc: 0.9915     \n",
      "Epoch 81/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.1453 - acc: 0.9880     \n",
      "Epoch 82/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1421 - acc: 0.9885     \n",
      "Epoch 83/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1428 - acc: 0.9910     \n",
      "Epoch 84/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1503 - acc: 0.9840     \n",
      "Epoch 85/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1414 - acc: 0.9920     \n",
      "Epoch 86/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.1445 - acc: 0.9895     \n",
      "Epoch 87/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1421 - acc: 0.9905     \n",
      "Epoch 88/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1426 - acc: 0.9890     \n",
      "Epoch 89/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1353 - acc: 0.9935     \n",
      "Epoch 90/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1418 - acc: 0.9905     \n",
      "Epoch 91/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1422 - acc: 0.9850     \n",
      "Epoch 92/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.1386 - acc: 0.9910     \n",
      "Epoch 93/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1390 - acc: 0.9915     \n",
      "Epoch 94/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1402 - acc: 0.9885     \n",
      "Epoch 95/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.1327 - acc: 0.9950     \n",
      "Epoch 96/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1393 - acc: 0.9905     \n",
      "Epoch 97/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1386 - acc: 0.9865     \n",
      "Epoch 98/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.1358 - acc: 0.9940     \n",
      "Epoch 99/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.1375 - acc: 0.9895     \n",
      "Epoch 100/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1430 - acc: 0.9860     \n",
      "Epoch 101/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1367 - acc: 0.9905     \n",
      "Epoch 102/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1378 - acc: 0.9905     \n",
      "Epoch 103/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1362 - acc: 0.9905     \n",
      "Epoch 104/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1318 - acc: 0.9950     \n",
      "Epoch 105/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1360 - acc: 0.9905     \n",
      "Epoch 106/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1367 - acc: 0.9900     \n",
      "Epoch 107/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1345 - acc: 0.9890     \n",
      "Epoch 108/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1329 - acc: 0.9910     \n",
      "Epoch 109/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1302 - acc: 0.9940     \n",
      "Epoch 110/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1366 - acc: 0.9880     \n",
      "Epoch 111/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1271 - acc: 0.9955     \n",
      "Epoch 112/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1288 - acc: 0.9930     \n",
      "Epoch 113/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1332 - acc: 0.9920     \n",
      "Epoch 114/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1338 - acc: 0.9900     \n",
      "Epoch 115/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1349 - acc: 0.9920     \n",
      "Epoch 116/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1326 - acc: 0.9920     \n",
      "Epoch 117/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1274 - acc: 0.9930     \n",
      "Epoch 118/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1342 - acc: 0.9920     \n",
      "Epoch 119/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1347 - acc: 0.9900     \n",
      "Epoch 120/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1327 - acc: 0.9915     \n",
      "Epoch 121/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1338 - acc: 0.9910     \n",
      "Epoch 122/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1272 - acc: 0.9925     \n",
      "Epoch 123/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1292 - acc: 0.9925     \n",
      "Epoch 124/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1277 - acc: 0.9950     \n",
      "Epoch 125/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1279 - acc: 0.9940     \n",
      "Epoch 126/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1278 - acc: 0.9925     \n",
      "Epoch 127/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1277 - acc: 0.9915     \n",
      "Epoch 128/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1295 - acc: 0.9920     \n",
      "Epoch 129/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1311 - acc: 0.9925     \n",
      "Epoch 130/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1272 - acc: 0.9930     \n",
      "Epoch 131/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1242 - acc: 0.9950     \n",
      "Epoch 132/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1281 - acc: 0.9920     \n",
      "Epoch 133/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1259 - acc: 0.9925     \n",
      "Epoch 134/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1312 - acc: 0.9895     \n",
      "Epoch 135/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1287 - acc: 0.9905     \n",
      "Epoch 136/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1311 - acc: 0.9885     \n",
      "Epoch 137/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1265 - acc: 0.9945     \n",
      "Epoch 138/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1245 - acc: 0.9945     \n",
      "Epoch 139/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1296 - acc: 0.9935     \n",
      "Epoch 140/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1242 - acc: 0.9950     \n",
      "Epoch 141/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1290 - acc: 0.9935     \n",
      "Epoch 142/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1269 - acc: 0.9920     \n",
      "Epoch 143/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1270 - acc: 0.9935     \n",
      "Epoch 144/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1259 - acc: 0.9930     \n",
      "Epoch 145/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1249 - acc: 0.9935     \n",
      "Epoch 146/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1263 - acc: 0.9930     \n",
      "Epoch 147/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1257 - acc: 0.9915     \n",
      "Epoch 148/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1295 - acc: 0.9910     \n",
      "Epoch 149/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1276 - acc: 0.9930     \n",
      "Epoch 150/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1213 - acc: 0.9970     \n",
      "Epoch 151/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1272 - acc: 0.9920     \n",
      "Epoch 152/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1273 - acc: 0.9915     \n",
      "Epoch 153/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1241 - acc: 0.9970     \n",
      "Epoch 154/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1260 - acc: 0.9930     \n",
      "Epoch 155/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1277 - acc: 0.9915     \n",
      "Epoch 156/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1262 - acc: 0.9935     \n",
      "Epoch 157/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1249 - acc: 0.9940     \n",
      "Epoch 158/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1228 - acc: 0.9955     \n",
      "Epoch 159/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1221 - acc: 0.9955     \n",
      "Epoch 160/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1252 - acc: 0.9915     \n",
      "Epoch 161/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1226 - acc: 0.9925     \n",
      "Epoch 162/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1239 - acc: 0.9905     \n",
      "Epoch 163/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1236 - acc: 0.9930     \n",
      "Epoch 164/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1249 - acc: 0.9920     \n",
      "Epoch 165/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1265 - acc: 0.9910     \n",
      "Epoch 166/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1232 - acc: 0.9935     \n",
      "Epoch 167/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1220 - acc: 0.9950     \n",
      "Epoch 168/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1232 - acc: 0.9945     \n",
      "Epoch 169/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1271 - acc: 0.9920     \n",
      "Epoch 170/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.1237 - acc: 0.9920     \n",
      "Epoch 171/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1258 - acc: 0.9930     \n",
      "Epoch 172/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1235 - acc: 0.9925     \n",
      "Epoch 173/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1201 - acc: 0.9960     \n",
      "Epoch 174/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1271 - acc: 0.9900     \n",
      "Epoch 175/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1258 - acc: 0.9905     \n",
      "Epoch 176/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1192 - acc: 0.9975     \n",
      "Epoch 177/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1212 - acc: 0.9960     \n",
      "Epoch 178/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1231 - acc: 0.9960     \n",
      "Epoch 179/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1231 - acc: 0.9940     \n",
      "Epoch 180/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1197 - acc: 0.9950     \n",
      "Epoch 181/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1235 - acc: 0.9940     \n",
      "Epoch 182/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1223 - acc: 0.9940     \n",
      "Epoch 183/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1217 - acc: 0.9960     \n",
      "Epoch 184/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1205 - acc: 0.9950     \n",
      "Epoch 185/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1229 - acc: 0.9940     \n",
      "Epoch 186/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1251 - acc: 0.9925     \n",
      "Epoch 187/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1197 - acc: 0.9965     \n",
      "Epoch 188/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1190 - acc: 0.9965     \n",
      "Epoch 189/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1196 - acc: 0.9965     \n",
      "Epoch 190/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1227 - acc: 0.9935     \n",
      "Epoch 191/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1259 - acc: 0.9950     \n",
      "Epoch 192/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1239 - acc: 0.9925     \n",
      "Epoch 193/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1191 - acc: 0.9955     \n",
      "Epoch 194/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1201 - acc: 0.9940     \n",
      "Epoch 195/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1228 - acc: 0.9940     \n",
      "Epoch 196/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1206 - acc: 0.9960     \n",
      "Epoch 197/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1210 - acc: 0.9955     \n",
      "Epoch 198/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1194 - acc: 0.9935     \n",
      "Epoch 199/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1221 - acc: 0.9930     \n",
      "Epoch 200/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1215 - acc: 0.9950     \n",
      "Epoch 201/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1186 - acc: 0.9955     \n",
      "Epoch 202/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1238 - acc: 0.9925     \n",
      "Epoch 203/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1232 - acc: 0.9935     \n",
      "Epoch 204/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1187 - acc: 0.9965     \n",
      "Epoch 205/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1222 - acc: 0.9945     \n",
      "Epoch 206/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1186 - acc: 0.9950     \n",
      "Epoch 207/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1177 - acc: 0.9955     \n",
      "Epoch 208/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1195 - acc: 0.9945     \n",
      "Epoch 209/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1204 - acc: 0.9950     \n",
      "Epoch 210/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1204 - acc: 0.9950     \n",
      "Epoch 211/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1196 - acc: 0.9955     \n",
      "Epoch 212/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1207 - acc: 0.9965     \n",
      "Epoch 213/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1201 - acc: 0.9960     \n",
      "Epoch 214/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1181 - acc: 0.9950     \n",
      "Epoch 215/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1199 - acc: 0.9940     \n",
      "Epoch 216/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1181 - acc: 0.9950     \n",
      "Epoch 217/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1199 - acc: 0.9955     \n",
      "Epoch 218/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1205 - acc: 0.9940     \n",
      "Epoch 219/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1169 - acc: 0.9965     \n",
      "Epoch 220/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1203 - acc: 0.9955     \n",
      "Epoch 221/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1201 - acc: 0.9950     \n",
      "Epoch 222/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1186 - acc: 0.9960     \n",
      "Epoch 223/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1195 - acc: 0.9955     \n",
      "Epoch 224/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1190 - acc: 0.9950     \n",
      "Epoch 225/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1210 - acc: 0.9950     \n",
      "Epoch 226/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1192 - acc: 0.9950     \n",
      "Epoch 227/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1166 - acc: 0.9965     \n",
      "Epoch 228/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1159 - acc: 0.9995     \n",
      "Epoch 229/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1163 - acc: 0.9975     \n",
      "Epoch 230/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1178 - acc: 0.9960     \n",
      "Epoch 231/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1156 - acc: 0.9980     \n",
      "Epoch 232/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1146 - acc: 0.9985     \n",
      "Epoch 233/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1206 - acc: 0.9930     \n",
      "Epoch 234/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1163 - acc: 0.9970     \n",
      "Epoch 235/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1208 - acc: 0.9965     \n",
      "Epoch 236/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1195 - acc: 0.9940     \n",
      "Epoch 237/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1167 - acc: 0.9965     \n",
      "Epoch 238/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1163 - acc: 0.9970     \n",
      "Epoch 239/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1201 - acc: 0.9925     \n",
      "Epoch 240/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1170 - acc: 0.9955     \n",
      "Epoch 241/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1187 - acc: 0.9955     \n",
      "Epoch 242/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1179 - acc: 0.9970     \n",
      "Epoch 243/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1180 - acc: 0.9965     \n",
      "Epoch 244/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1158 - acc: 0.9965     \n",
      "Epoch 245/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1184 - acc: 0.9965     \n",
      "Epoch 246/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1175 - acc: 0.9950     \n",
      "Epoch 247/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1184 - acc: 0.9940     \n",
      "Epoch 248/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1191 - acc: 0.9945     \n",
      "Epoch 249/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1178 - acc: 0.9950     \n",
      "Epoch 250/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1139 - acc: 0.9975     \n",
      "Epoch 251/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1155 - acc: 0.9970     \n",
      "Epoch 252/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1157 - acc: 0.9985     \n",
      "Epoch 253/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1176 - acc: 0.9955     \n",
      "Epoch 254/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.1155 - acc: 0.9970     \n",
      "Epoch 255/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1169 - acc: 0.9960     \n",
      "Epoch 256/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1149 - acc: 0.9970     \n",
      "Epoch 257/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1157 - acc: 0.9970     \n",
      "Epoch 258/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1158 - acc: 0.9960     \n",
      "Epoch 259/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1179 - acc: 0.9950     \n",
      "Epoch 260/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1159 - acc: 0.9965     \n",
      "Epoch 261/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1191 - acc: 0.9970     \n",
      "Epoch 262/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1167 - acc: 0.9955     \n",
      "Epoch 263/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1149 - acc: 0.9965     \n",
      "Epoch 264/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1154 - acc: 0.9980     \n",
      "Epoch 265/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1128 - acc: 0.9975     \n",
      "Epoch 266/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1130 - acc: 0.9985     \n",
      "Epoch 267/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1151 - acc: 0.9945     \n",
      "Epoch 268/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1131 - acc: 0.9980     \n",
      "Epoch 269/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1155 - acc: 0.9955     \n",
      "Epoch 270/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1123 - acc: 0.9985     \n",
      "Epoch 271/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1157 - acc: 0.9970     \n",
      "Epoch 272/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1143 - acc: 0.9975     \n",
      "Epoch 273/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1143 - acc: 0.9970     \n",
      "Epoch 274/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1134 - acc: 0.9965     \n",
      "Epoch 275/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1163 - acc: 0.9975     \n",
      "Epoch 276/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1151 - acc: 0.9970     \n",
      "Epoch 277/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1163 - acc: 0.9970     \n",
      "Epoch 278/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1156 - acc: 0.9960     \n",
      "Epoch 279/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1150 - acc: 0.9980     \n",
      "Epoch 280/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1133 - acc: 0.9965     \n",
      "Epoch 281/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1142 - acc: 0.9965     \n",
      "Epoch 282/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1136 - acc: 0.9980     \n",
      "Epoch 283/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1138 - acc: 0.9965     \n",
      "Epoch 284/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1147 - acc: 0.9945     \n",
      "Epoch 285/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1159 - acc: 0.9965     \n",
      "Epoch 286/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1145 - acc: 0.9970     \n",
      "Epoch 287/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1150 - acc: 0.9965     \n",
      "Epoch 288/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1128 - acc: 0.9985     \n",
      "Epoch 289/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1121 - acc: 0.9985     \n",
      "Epoch 290/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1116 - acc: 0.9990     \n",
      "Epoch 291/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1145 - acc: 0.9980     \n",
      "Epoch 292/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1128 - acc: 0.9990     \n",
      "Epoch 293/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1118 - acc: 0.9990     \n",
      "Epoch 294/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1127 - acc: 0.9980     \n",
      "Epoch 295/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1154 - acc: 0.9965     \n",
      "Epoch 296/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1160 - acc: 0.9960     \n",
      "Epoch 297/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1155 - acc: 0.9975     \n",
      "Epoch 298/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1150 - acc: 0.9965     \n",
      "Epoch 299/1000\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.1113 - acc: 0.998 - 1s - loss: 0.1117 - acc: 0.9980     \n",
      "Epoch 300/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1154 - acc: 0.9970     \n",
      "Epoch 301/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1143 - acc: 0.9965     \n",
      "Epoch 302/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1124 - acc: 0.9965     \n",
      "Epoch 303/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1116 - acc: 0.9965     \n",
      "Epoch 304/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1152 - acc: 0.9980     \n",
      "Epoch 305/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1127 - acc: 0.9975     \n",
      "Epoch 306/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1144 - acc: 0.9975     \n",
      "Epoch 307/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1147 - acc: 0.9965     \n",
      "Epoch 308/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1123 - acc: 0.9980     \n",
      "Epoch 309/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1148 - acc: 0.9985     \n",
      "Epoch 310/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1129 - acc: 0.9970     \n",
      "Epoch 311/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1127 - acc: 0.9985     \n",
      "Epoch 312/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1145 - acc: 0.9970     \n",
      "Epoch 313/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1136 - acc: 0.9980     \n",
      "Epoch 314/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1144 - acc: 0.9980     \n",
      "Epoch 315/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1112 - acc: 0.9970     \n",
      "Epoch 316/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1134 - acc: 0.9980     \n",
      "Epoch 317/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1113 - acc: 0.9985     \n",
      "Epoch 318/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1127 - acc: 0.9980     \n",
      "Epoch 319/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1114 - acc: 0.9980     \n",
      "Epoch 320/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1146 - acc: 0.9950     \n",
      "Epoch 321/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1123 - acc: 0.9970     \n",
      "Epoch 322/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1124 - acc: 0.9975     \n",
      "Epoch 323/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1103 - acc: 0.9990     \n",
      "Epoch 324/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1119 - acc: 0.9980     \n",
      "Epoch 325/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1124 - acc: 0.9955     \n",
      "Epoch 326/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1121 - acc: 0.9975     \n",
      "Epoch 327/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1139 - acc: 0.9980     \n",
      "Epoch 328/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1107 - acc: 0.9980     \n",
      "Epoch 329/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1111 - acc: 0.9985     \n",
      "Epoch 330/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1113 - acc: 0.9975     \n",
      "Epoch 331/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1117 - acc: 0.9990     \n",
      "Epoch 332/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1143 - acc: 0.9955     \n",
      "Epoch 333/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1124 - acc: 0.9960     \n",
      "Epoch 334/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1115 - acc: 0.9980     \n",
      "Epoch 335/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1090 - acc: 0.9990     \n",
      "Epoch 336/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1119 - acc: 0.9975     \n",
      "Epoch 337/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1128 - acc: 0.9980     \n",
      "Epoch 338/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.1113 - acc: 0.9975     \n",
      "Epoch 339/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1116 - acc: 0.9985     \n",
      "Epoch 340/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1117 - acc: 0.9965     \n",
      "Epoch 341/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1125 - acc: 0.9975     \n",
      "Epoch 342/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1145 - acc: 0.9960     \n",
      "Epoch 343/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1125 - acc: 0.9950     \n",
      "Epoch 344/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1120 - acc: 0.9980     \n",
      "Epoch 345/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1138 - acc: 0.9965     \n",
      "Epoch 346/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1135 - acc: 0.9965     \n",
      "Epoch 347/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1102 - acc: 0.9990     \n",
      "Epoch 348/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1101 - acc: 0.9985     \n",
      "Epoch 349/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1095 - acc: 0.9980     \n",
      "Epoch 350/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1117 - acc: 0.9990     \n",
      "Epoch 351/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1104 - acc: 0.9980     \n",
      "Epoch 352/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1109 - acc: 0.9975     \n",
      "Epoch 353/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1088 - acc: 0.9995     \n",
      "Epoch 354/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1095 - acc: 0.9990     \n",
      "Epoch 355/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1111 - acc: 0.9980     \n",
      "Epoch 356/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1116 - acc: 0.9980     \n",
      "Epoch 357/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1107 - acc: 0.9970     \n",
      "Epoch 358/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1102 - acc: 0.9985     \n",
      "Epoch 359/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1107 - acc: 0.9980     \n",
      "Epoch 360/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1121 - acc: 0.9960     \n",
      "Epoch 361/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1091 - acc: 0.9985     \n",
      "Epoch 362/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1119 - acc: 0.9970     \n",
      "Epoch 363/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1118 - acc: 0.9970     \n",
      "Epoch 364/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1093 - acc: 0.9990     \n",
      "Epoch 365/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1117 - acc: 0.9970     \n",
      "Epoch 366/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1084 - acc: 0.9995     \n",
      "Epoch 367/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1116 - acc: 0.9985     \n",
      "Epoch 368/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1112 - acc: 0.9975     \n",
      "Epoch 369/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1076 - acc: 0.9985     \n",
      "Epoch 370/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1093 - acc: 0.9980     \n",
      "Epoch 371/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1107 - acc: 0.9970     \n",
      "Epoch 372/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1100 - acc: 0.9990     \n",
      "Epoch 373/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1078 - acc: 0.9990     \n",
      "Epoch 374/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1093 - acc: 0.9985     \n",
      "Epoch 375/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1083 - acc: 0.9985     \n",
      "Epoch 376/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1093 - acc: 0.9995     \n",
      "Epoch 377/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1091 - acc: 0.9985     \n",
      "Epoch 378/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1106 - acc: 0.9980     \n",
      "Epoch 379/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1104 - acc: 0.9970     \n",
      "Epoch 380/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1084 - acc: 0.9985     \n",
      "Epoch 381/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1104 - acc: 0.9980     \n",
      "Epoch 382/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1104 - acc: 0.9975     \n",
      "Epoch 383/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1100 - acc: 0.9985     \n",
      "Epoch 384/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1081 - acc: 0.9985     \n",
      "Epoch 385/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1086 - acc: 0.9985     \n",
      "Epoch 386/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1082 - acc: 0.9985     \n",
      "Epoch 387/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1082 - acc: 0.9990     \n",
      "Epoch 388/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1093 - acc: 0.9980     \n",
      "Epoch 389/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1090 - acc: 0.9990     \n",
      "Epoch 390/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1097 - acc: 0.9985     \n",
      "Epoch 391/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1104 - acc: 0.9965     \n",
      "Epoch 392/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1098 - acc: 0.9975     \n",
      "Epoch 393/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1079 - acc: 0.9990     \n",
      "Epoch 394/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1084 - acc: 0.9980     \n",
      "Epoch 395/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1101 - acc: 0.9980     \n",
      "Epoch 396/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1112 - acc: 0.9955     \n",
      "Epoch 397/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1081 - acc: 0.9995     \n",
      "Epoch 398/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1086 - acc: 0.9985     \n",
      "Epoch 399/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1088 - acc: 0.9985     \n",
      "Epoch 400/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1086 - acc: 0.9995     \n",
      "Epoch 401/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1092 - acc: 0.9985     \n",
      "Epoch 402/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1076 - acc: 0.9985     \n",
      "Epoch 403/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1101 - acc: 0.9980     \n",
      "Epoch 404/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1081 - acc: 0.9990     \n",
      "Epoch 405/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1082 - acc: 0.9995     \n",
      "Epoch 406/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1080 - acc: 0.9985     \n",
      "Epoch 407/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1075 - acc: 0.9980     \n",
      "Epoch 408/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1090 - acc: 0.9990     \n",
      "Epoch 409/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1106 - acc: 0.9975     \n",
      "Epoch 410/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1099 - acc: 0.9980     \n",
      "Epoch 411/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1073 - acc: 0.9990     \n",
      "Epoch 412/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1072 - acc: 0.9990     \n",
      "Epoch 413/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1086 - acc: 0.9995     \n",
      "Epoch 414/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1081 - acc: 0.9980     \n",
      "Epoch 415/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1068 - acc: 0.9990     \n",
      "Epoch 416/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1083 - acc: 0.9975     \n",
      "Epoch 417/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1098 - acc: 0.9985     \n",
      "Epoch 418/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1074 - acc: 0.9985     \n",
      "Epoch 419/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1074 - acc: 0.9995     \n",
      "Epoch 420/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1089 - acc: 0.9985     \n",
      "Epoch 421/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1071 - acc: 0.9990     \n",
      "Epoch 422/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.1066 - acc: 0.9995     \n",
      "Epoch 423/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1086 - acc: 0.9985     \n",
      "Epoch 424/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1091 - acc: 0.9975     \n",
      "Epoch 425/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1079 - acc: 0.9995     \n",
      "Epoch 426/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1078 - acc: 0.9990     \n",
      "Epoch 427/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1077 - acc: 0.9980     \n",
      "Epoch 428/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1069 - acc: 0.9990     \n",
      "Epoch 429/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1079 - acc: 0.9975     \n",
      "Epoch 430/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1068 - acc: 0.9990     \n",
      "Epoch 431/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1078 - acc: 0.9980     \n",
      "Epoch 432/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1070 - acc: 0.9995     \n",
      "Epoch 433/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1086 - acc: 0.9980     \n",
      "Epoch 434/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1085 - acc: 0.9980     \n",
      "Epoch 435/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1087 - acc: 0.9980     \n",
      "Epoch 436/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1059 - acc: 1.0000     \n",
      "Epoch 437/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.1078 - acc: 0.9975     \n",
      "Epoch 438/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1061 - acc: 0.9985     \n",
      "Epoch 439/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1059 - acc: 0.9990     \n",
      "Epoch 440/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.1063 - acc: 0.9985     \n",
      "Epoch 441/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.1073 - acc: 0.9980     \n",
      "Epoch 442/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.1061 - acc: 0.9985     \n",
      "Epoch 443/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1067 - acc: 0.9990     \n",
      "Epoch 444/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1065 - acc: 0.9990     \n",
      "Epoch 445/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1082 - acc: 0.9980     \n",
      "Epoch 446/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1057 - acc: 0.9985     \n",
      "Epoch 447/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1052 - acc: 0.9990     \n",
      "Epoch 448/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1062 - acc: 0.9990     \n",
      "Epoch 449/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1061 - acc: 0.9990     \n",
      "Epoch 450/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1051 - acc: 0.9995     \n",
      "Epoch 451/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1045 - acc: 0.9995     \n",
      "Epoch 452/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1081 - acc: 0.9970     \n",
      "Epoch 453/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1052 - acc: 0.9995     \n",
      "Epoch 454/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1059 - acc: 0.9995     \n",
      "Epoch 455/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1057 - acc: 0.9995     \n",
      "Epoch 456/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1069 - acc: 0.9985     \n",
      "Epoch 457/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1061 - acc: 0.9985     \n",
      "Epoch 458/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1050 - acc: 0.9990     \n",
      "Epoch 459/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1066 - acc: 0.9985     \n",
      "Epoch 460/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1062 - acc: 0.9980     \n",
      "Epoch 461/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1046 - acc: 0.9980     \n",
      "Epoch 462/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1079 - acc: 0.9980     \n",
      "Epoch 463/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1045 - acc: 0.9995     \n",
      "Epoch 464/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1072 - acc: 0.9995     \n",
      "Epoch 465/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1052 - acc: 0.9995     \n",
      "Epoch 466/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1058 - acc: 0.9995     \n",
      "Epoch 467/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1059 - acc: 0.9995     \n",
      "Epoch 468/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1056 - acc: 0.9995     \n",
      "Epoch 469/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1049 - acc: 0.9990     \n",
      "Epoch 470/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1080 - acc: 0.9975     \n",
      "Epoch 471/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1065 - acc: 0.9975     \n",
      "Epoch 472/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1053 - acc: 0.9995     \n",
      "Epoch 473/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1049 - acc: 0.9995     \n",
      "Epoch 474/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1056 - acc: 0.9980     \n",
      "Epoch 475/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1042 - acc: 1.0000     \n",
      "Epoch 476/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1055 - acc: 0.9995     \n",
      "Epoch 477/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1047 - acc: 1.0000     \n",
      "Epoch 478/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1079 - acc: 0.9970     \n",
      "Epoch 479/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1053 - acc: 0.9990     \n",
      "Epoch 480/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1054 - acc: 0.9995     \n",
      "Epoch 481/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1055 - acc: 0.9995     \n",
      "Epoch 482/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1060 - acc: 0.9985     \n",
      "Epoch 483/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1046 - acc: 0.9990     \n",
      "Epoch 484/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1066 - acc: 0.9985     \n",
      "Epoch 485/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1045 - acc: 0.9995     \n",
      "Epoch 486/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1044 - acc: 0.9990     \n",
      "Epoch 487/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1042 - acc: 0.9995     \n",
      "Epoch 488/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1059 - acc: 0.9985     \n",
      "Epoch 489/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1050 - acc: 0.9990     \n",
      "Epoch 490/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1058 - acc: 0.9985     \n",
      "Epoch 491/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1043 - acc: 0.9985     \n",
      "Epoch 492/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1074 - acc: 0.9980     \n",
      "Epoch 493/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1041 - acc: 0.9995     \n",
      "Epoch 494/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1052 - acc: 1.0000     \n",
      "Epoch 495/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1045 - acc: 0.9995     \n",
      "Epoch 496/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1042 - acc: 0.9975     \n",
      "Epoch 497/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1050 - acc: 0.9990     \n",
      "Epoch 498/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1043 - acc: 0.9985     \n",
      "Epoch 499/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1042 - acc: 0.9985     \n",
      "Epoch 500/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1046 - acc: 0.9995     \n",
      "Epoch 501/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1040 - acc: 0.9995     \n",
      "Epoch 502/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1058 - acc: 0.9970     \n",
      "Epoch 503/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1044 - acc: 1.0000     \n",
      "Epoch 504/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1042 - acc: 0.9990     \n",
      "Epoch 505/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1039 - acc: 0.9995     \n",
      "Epoch 506/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.1045 - acc: 0.9995     \n",
      "Epoch 507/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1055 - acc: 0.9995     \n",
      "Epoch 508/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1039 - acc: 0.9990     \n",
      "Epoch 509/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1040 - acc: 0.9985     \n",
      "Epoch 510/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1049 - acc: 0.9990     \n",
      "Epoch 511/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1039 - acc: 0.9985     \n",
      "Epoch 512/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1045 - acc: 0.9985     \n",
      "Epoch 513/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1046 - acc: 0.9990     \n",
      "Epoch 514/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1036 - acc: 1.0000     \n",
      "Epoch 515/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1028 - acc: 1.0000     \n",
      "Epoch 516/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1041 - acc: 0.9985     \n",
      "Epoch 517/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1043 - acc: 0.9980     \n",
      "Epoch 518/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1042 - acc: 0.9980     \n",
      "Epoch 519/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1036 - acc: 0.9990     \n",
      "Epoch 520/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1027 - acc: 0.9990     \n",
      "Epoch 521/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1048 - acc: 0.9985     \n",
      "Epoch 522/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1022 - acc: 0.9990     \n",
      "Epoch 523/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1035 - acc: 0.9995     \n",
      "Epoch 524/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1025 - acc: 0.9995     \n",
      "Epoch 525/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1021 - acc: 0.9995     \n",
      "Epoch 526/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1030 - acc: 1.0000     \n",
      "Epoch 527/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1036 - acc: 0.9990     \n",
      "Epoch 528/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1032 - acc: 0.9995     \n",
      "Epoch 529/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1020 - acc: 0.9995     \n",
      "Epoch 530/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1030 - acc: 0.9990     \n",
      "Epoch 531/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1020 - acc: 1.0000     \n",
      "Epoch 532/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1049 - acc: 0.9985     \n",
      "Epoch 533/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1022 - acc: 0.9995     \n",
      "Epoch 534/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1037 - acc: 0.9995     \n",
      "Epoch 535/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1029 - acc: 0.9985     \n",
      "Epoch 536/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1035 - acc: 0.9980     \n",
      "Epoch 537/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1024 - acc: 0.9995     \n",
      "Epoch 538/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1023 - acc: 0.9990     \n",
      "Epoch 539/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1029 - acc: 0.9990     \n",
      "Epoch 540/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1032 - acc: 0.9990     \n",
      "Epoch 541/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1027 - acc: 0.9995     \n",
      "Epoch 542/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1026 - acc: 0.9990     \n",
      "Epoch 543/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1039 - acc: 0.9980     \n",
      "Epoch 544/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1012 - acc: 0.9995     \n",
      "Epoch 545/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1034 - acc: 0.9990     \n",
      "Epoch 546/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1020 - acc: 0.9995     \n",
      "Epoch 547/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1043 - acc: 0.9975     \n",
      "Epoch 548/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1030 - acc: 0.9990     \n",
      "Epoch 549/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1024 - acc: 0.9990     \n",
      "Epoch 550/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1019 - acc: 1.0000     \n",
      "Epoch 551/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1042 - acc: 0.9980     \n",
      "Epoch 552/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1035 - acc: 0.9990     \n",
      "Epoch 553/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1019 - acc: 0.9995     \n",
      "Epoch 554/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1034 - acc: 0.9990     \n",
      "Epoch 555/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1020 - acc: 0.9995     \n",
      "Epoch 556/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1043 - acc: 0.9980     \n",
      "Epoch 557/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1025 - acc: 0.9990     \n",
      "Epoch 558/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1042 - acc: 0.9990     \n",
      "Epoch 559/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1036 - acc: 0.9990     \n",
      "Epoch 560/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1017 - acc: 0.9990     \n",
      "Epoch 561/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1016 - acc: 0.9995     \n",
      "Epoch 562/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1013 - acc: 1.0000     \n",
      "Epoch 563/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1034 - acc: 0.9995     \n",
      "Epoch 564/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1027 - acc: 0.9995     \n",
      "Epoch 565/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1009 - acc: 0.9995     \n",
      "Epoch 566/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1022 - acc: 1.0000     \n",
      "Epoch 567/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1034 - acc: 0.9990     \n",
      "Epoch 568/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1028 - acc: 1.0000     \n",
      "Epoch 569/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1022 - acc: 0.9995     \n",
      "Epoch 570/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1025 - acc: 0.9995     \n",
      "Epoch 571/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1032 - acc: 0.9990     \n",
      "Epoch 572/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1024 - acc: 0.9990     \n",
      "Epoch 573/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1017 - acc: 0.9990     \n",
      "Epoch 574/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1008 - acc: 0.9990     \n",
      "Epoch 575/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1032 - acc: 0.9990     \n",
      "Epoch 576/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1020 - acc: 1.0000     \n",
      "Epoch 577/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1027 - acc: 0.9990     \n",
      "Epoch 578/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1012 - acc: 0.9995     \n",
      "Epoch 579/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1029 - acc: 0.9990     \n",
      "Epoch 580/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1007 - acc: 1.0000     \n",
      "Epoch 581/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1025 - acc: 0.9990     \n",
      "Epoch 582/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1025 - acc: 0.9995     \n",
      "Epoch 583/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1029 - acc: 0.9985     \n",
      "Epoch 584/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1006 - acc: 0.9990     \n",
      "Epoch 585/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1002 - acc: 0.9995     \n",
      "Epoch 586/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1019 - acc: 0.9995     \n",
      "Epoch 587/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1012 - acc: 0.9990     \n",
      "Epoch 588/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1017 - acc: 0.9995     \n",
      "Epoch 589/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1022 - acc: 0.9995     \n",
      "Epoch 590/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.1007 - acc: 1.0000     \n",
      "Epoch 591/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1019 - acc: 0.9980     \n",
      "Epoch 592/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1005 - acc: 0.9995     \n",
      "Epoch 593/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1013 - acc: 0.9990     \n",
      "Epoch 594/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1002 - acc: 1.0000     \n",
      "Epoch 595/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1001 - acc: 1.0000     \n",
      "Epoch 596/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1002 - acc: 0.9995     \n",
      "Epoch 597/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1009 - acc: 0.9990     \n",
      "Epoch 598/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1007 - acc: 0.9990     \n",
      "Epoch 599/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1014 - acc: 0.9990     \n",
      "Epoch 600/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1029 - acc: 0.9975     \n",
      "Epoch 601/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1005 - acc: 0.9990     \n",
      "Epoch 602/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1024 - acc: 0.9990     \n",
      "Epoch 603/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1012 - acc: 0.9990     \n",
      "Epoch 604/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1017 - acc: 1.0000     \n",
      "Epoch 605/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1001 - acc: 0.9990     \n",
      "Epoch 606/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1008 - acc: 0.9990     \n",
      "Epoch 607/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1024 - acc: 0.9990     \n",
      "Epoch 608/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1001 - acc: 1.0000     \n",
      "Epoch 609/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1019 - acc: 0.9985     \n",
      "Epoch 610/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0997 - acc: 0.9995     \n",
      "Epoch 611/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1025 - acc: 0.9985     \n",
      "Epoch 612/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1001 - acc: 0.9990     \n",
      "Epoch 613/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1001 - acc: 0.9995     \n",
      "Epoch 614/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1004 - acc: 1.0000     \n",
      "Epoch 615/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1001 - acc: 0.9995     \n",
      "Epoch 616/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1024 - acc: 0.9985     \n",
      "Epoch 617/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1000 - acc: 0.9985     \n",
      "Epoch 618/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0997 - acc: 0.9990     \n",
      "Epoch 619/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1006 - acc: 0.9985     \n",
      "Epoch 620/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1001 - acc: 0.9995     \n",
      "Epoch 621/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0988 - acc: 0.9995     \n",
      "Epoch 622/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0999 - acc: 0.9995     \n",
      "Epoch 623/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1018 - acc: 0.9980     \n",
      "Epoch 624/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1005 - acc: 0.9985     \n",
      "Epoch 625/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1004 - acc: 0.9990     \n",
      "Epoch 626/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1009 - acc: 0.9990     \n",
      "Epoch 627/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1006 - acc: 0.9985     \n",
      "Epoch 628/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1001 - acc: 0.9990     \n",
      "Epoch 629/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1000 - acc: 1.0000     \n",
      "Epoch 630/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1004 - acc: 0.9980     \n",
      "Epoch 631/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1012 - acc: 0.9985     \n",
      "Epoch 632/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0998 - acc: 0.9990     \n",
      "Epoch 633/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1003 - acc: 0.9995     \n",
      "Epoch 634/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1018 - acc: 0.9990     \n",
      "Epoch 635/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1011 - acc: 0.9975     \n",
      "Epoch 636/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0988 - acc: 1.0000     \n",
      "Epoch 637/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1001 - acc: 0.9985     \n",
      "Epoch 638/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1000 - acc: 0.9985     \n",
      "Epoch 639/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0989 - acc: 0.9995     \n",
      "Epoch 640/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0998 - acc: 0.9995     \n",
      "Epoch 641/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1004 - acc: 0.9985     \n",
      "Epoch 642/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0988 - acc: 0.9995     \n",
      "Epoch 643/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0986 - acc: 1.0000     \n",
      "Epoch 644/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1002 - acc: 0.9990     \n",
      "Epoch 645/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0994 - acc: 0.9995     \n",
      "Epoch 646/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0998 - acc: 0.9995     \n",
      "Epoch 647/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0988 - acc: 1.0000     \n",
      "Epoch 648/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1007 - acc: 0.9985     \n",
      "Epoch 649/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1007 - acc: 0.9985     \n",
      "Epoch 650/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0998 - acc: 0.9995     \n",
      "Epoch 651/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0996 - acc: 1.0000     \n",
      "Epoch 652/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0988 - acc: 0.9990     \n",
      "Epoch 653/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0993 - acc: 0.9990     \n",
      "Epoch 654/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1004 - acc: 0.9985     \n",
      "Epoch 655/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0989 - acc: 1.0000     \n",
      "Epoch 656/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1003 - acc: 0.9990     \n",
      "Epoch 657/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0994 - acc: 0.9995     \n",
      "Epoch 658/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0989 - acc: 0.9995     \n",
      "Epoch 659/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0988 - acc: 1.0000     \n",
      "Epoch 660/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0983 - acc: 1.0000     \n",
      "Epoch 661/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0990 - acc: 0.9990     \n",
      "Epoch 662/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0995 - acc: 0.9990     \n",
      "Epoch 663/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0992 - acc: 1.0000     \n",
      "Epoch 664/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1007 - acc: 0.9990     \n",
      "Epoch 665/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0989 - acc: 0.9995     \n",
      "Epoch 666/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1003 - acc: 0.9990     \n",
      "Epoch 667/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0991 - acc: 0.9990     \n",
      "Epoch 668/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0980 - acc: 1.0000     \n",
      "Epoch 669/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0994 - acc: 0.9985     \n",
      "Epoch 670/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0980 - acc: 1.0000     \n",
      "Epoch 671/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0991 - acc: 1.0000     \n",
      "Epoch 672/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1003 - acc: 0.9995     \n",
      "Epoch 673/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0996 - acc: 0.9990     \n",
      "Epoch 674/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.0998 - acc: 0.9985     \n",
      "Epoch 675/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0974 - acc: 0.9995     \n",
      "Epoch 676/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0988 - acc: 0.9990     \n",
      "Epoch 677/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0986 - acc: 0.9995     \n",
      "Epoch 678/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0998 - acc: 0.9990     \n",
      "Epoch 679/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1010 - acc: 0.9980     \n",
      "Epoch 680/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0989 - acc: 0.9995     \n",
      "Epoch 681/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0988 - acc: 0.9985     \n",
      "Epoch 682/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0994 - acc: 0.9990     \n",
      "Epoch 683/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0986 - acc: 0.9985     \n",
      "Epoch 684/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0981 - acc: 1.0000     \n",
      "Epoch 685/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1006 - acc: 0.9990     \n",
      "Epoch 686/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0975 - acc: 0.9995     \n",
      "Epoch 687/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0972 - acc: 0.9995     \n",
      "Epoch 688/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0981 - acc: 0.9995     \n",
      "Epoch 689/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0980 - acc: 0.9995     \n",
      "Epoch 690/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1011 - acc: 0.9970     - ETA: 1s - \n",
      "Epoch 691/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0986 - acc: 0.9990     \n",
      "Epoch 692/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0978 - acc: 1.0000     \n",
      "Epoch 693/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0981 - acc: 0.9990     \n",
      "Epoch 694/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0996 - acc: 0.9985     \n",
      "Epoch 695/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0969 - acc: 1.0000     \n",
      "Epoch 696/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0983 - acc: 0.9985     \n",
      "Epoch 697/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0990 - acc: 0.9985     \n",
      "Epoch 698/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0978 - acc: 0.9990     \n",
      "Epoch 699/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0983 - acc: 0.9990     \n",
      "Epoch 700/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0978 - acc: 0.9995     \n",
      "Epoch 701/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0986 - acc: 0.9990     \n",
      "Epoch 702/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0985 - acc: 0.9990     \n",
      "Epoch 703/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0983 - acc: 0.9995     \n",
      "Epoch 704/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1002 - acc: 0.9980     \n",
      "Epoch 705/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0994 - acc: 0.9985     \n",
      "Epoch 706/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0971 - acc: 1.0000     \n",
      "Epoch 707/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0983 - acc: 1.0000     \n",
      "Epoch 708/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0962 - acc: 1.0000     \n",
      "Epoch 709/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0970 - acc: 0.9995     \n",
      "Epoch 710/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0970 - acc: 1.0000     \n",
      "Epoch 711/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0981 - acc: 0.9995     \n",
      "Epoch 712/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0986 - acc: 0.9980     \n",
      "Epoch 713/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0985 - acc: 0.9985     \n",
      "Epoch 714/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0970 - acc: 1.0000     \n",
      "Epoch 715/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0975 - acc: 1.0000     \n",
      "Epoch 716/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0980 - acc: 0.9990     \n",
      "Epoch 717/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0976 - acc: 0.9990     \n",
      "Epoch 718/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0976 - acc: 0.9995     \n",
      "Epoch 719/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0972 - acc: 0.9995     \n",
      "Epoch 720/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0976 - acc: 1.0000     \n",
      "Epoch 721/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0960 - acc: 1.0000     \n",
      "Epoch 722/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0980 - acc: 0.9995     \n",
      "Epoch 723/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0970 - acc: 0.9990     \n",
      "Epoch 724/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0978 - acc: 0.9990     \n",
      "Epoch 725/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.1001 - acc: 0.9980     \n",
      "Epoch 726/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0976 - acc: 0.9990     \n",
      "Epoch 727/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0975 - acc: 0.9990     \n",
      "Epoch 728/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0967 - acc: 1.0000     \n",
      "Epoch 729/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0978 - acc: 1.0000     \n",
      "Epoch 730/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0983 - acc: 0.9985     \n",
      "Epoch 731/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0967 - acc: 1.0000     \n",
      "Epoch 732/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0964 - acc: 0.9995     \n",
      "Epoch 733/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0968 - acc: 0.9990     \n",
      "Epoch 734/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0982 - acc: 0.9995     \n",
      "Epoch 735/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0978 - acc: 0.9980     \n",
      "Epoch 736/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0975 - acc: 0.9995     \n",
      "Epoch 737/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0980 - acc: 0.9980     \n",
      "Epoch 738/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0970 - acc: 0.9995     \n",
      "Epoch 739/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0964 - acc: 0.9995     \n",
      "Epoch 740/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0964 - acc: 1.0000     \n",
      "Epoch 741/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0968 - acc: 0.9995     \n",
      "Epoch 742/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0968 - acc: 1.0000     \n",
      "Epoch 743/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0965 - acc: 0.9995     \n",
      "Epoch 744/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0979 - acc: 0.9995     \n",
      "Epoch 745/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0974 - acc: 0.9995     \n",
      "Epoch 746/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0971 - acc: 0.9990     \n",
      "Epoch 747/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0966 - acc: 0.9995     \n",
      "Epoch 748/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0966 - acc: 0.9990     \n",
      "Epoch 749/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0972 - acc: 0.9995     \n",
      "Epoch 750/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0982 - acc: 0.9980     \n",
      "Epoch 751/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0961 - acc: 1.0000     \n",
      "Epoch 752/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0960 - acc: 0.9990     \n",
      "Epoch 753/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0965 - acc: 0.9990     \n",
      "Epoch 754/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0975 - acc: 0.9990     \n",
      "Epoch 755/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0969 - acc: 1.0000     \n",
      "Epoch 756/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0967 - acc: 0.9995     \n",
      "Epoch 757/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0958 - acc: 1.0000     \n",
      "Epoch 758/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.0975 - acc: 0.9985     \n",
      "Epoch 759/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0958 - acc: 1.0000     \n",
      "Epoch 760/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0964 - acc: 1.0000     \n",
      "Epoch 761/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0974 - acc: 0.9980     \n",
      "Epoch 762/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0966 - acc: 1.0000     \n",
      "Epoch 763/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0964 - acc: 0.9990     \n",
      "Epoch 764/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0956 - acc: 1.0000     \n",
      "Epoch 765/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0977 - acc: 0.9975     \n",
      "Epoch 766/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0965 - acc: 0.9995     \n",
      "Epoch 767/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0969 - acc: 0.9990     \n",
      "Epoch 768/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0957 - acc: 1.0000     \n",
      "Epoch 769/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0952 - acc: 1.0000     \n",
      "Epoch 770/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0977 - acc: 0.9995     \n",
      "Epoch 771/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0956 - acc: 1.0000     \n",
      "Epoch 772/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0953 - acc: 0.9990     \n",
      "Epoch 773/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0969 - acc: 0.9995     \n",
      "Epoch 774/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0953 - acc: 1.0000     \n",
      "Epoch 775/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0957 - acc: 1.0000     \n",
      "Epoch 776/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0958 - acc: 1.0000     \n",
      "Epoch 777/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0953 - acc: 0.9990     \n",
      "Epoch 778/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0949 - acc: 1.0000     \n",
      "Epoch 779/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0963 - acc: 0.9990     \n",
      "Epoch 780/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0959 - acc: 0.9985     \n",
      "Epoch 781/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0954 - acc: 0.9995     \n",
      "Epoch 782/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0958 - acc: 0.9995     \n",
      "Epoch 783/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0960 - acc: 1.0000     \n",
      "Epoch 784/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0960 - acc: 0.9995     \n",
      "Epoch 785/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0958 - acc: 0.9995     \n",
      "Epoch 786/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0959 - acc: 0.9995     \n",
      "Epoch 787/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0953 - acc: 0.9995     \n",
      "Epoch 788/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0949 - acc: 0.9995     \n",
      "Epoch 789/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0954 - acc: 1.0000     \n",
      "Epoch 790/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0954 - acc: 1.0000     \n",
      "Epoch 791/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0955 - acc: 0.9990     \n",
      "Epoch 792/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0960 - acc: 0.9990     \n",
      "Epoch 793/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0953 - acc: 1.0000     \n",
      "Epoch 794/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0959 - acc: 0.9995     \n",
      "Epoch 795/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0961 - acc: 0.9985     \n",
      "Epoch 796/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0949 - acc: 1.0000     \n",
      "Epoch 797/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0958 - acc: 0.9995     \n",
      "Epoch 798/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0949 - acc: 1.0000     \n",
      "Epoch 799/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0950 - acc: 0.9995     \n",
      "Epoch 800/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0954 - acc: 0.9995     \n",
      "Epoch 801/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0948 - acc: 1.0000     \n",
      "Epoch 802/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0940 - acc: 1.0000     \n",
      "Epoch 803/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0959 - acc: 0.9995     \n",
      "Epoch 804/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0954 - acc: 0.9995     \n",
      "Epoch 805/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0947 - acc: 0.9995     \n",
      "Epoch 806/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0943 - acc: 0.9995     \n",
      "Epoch 807/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0970 - acc: 0.9980     \n",
      "Epoch 808/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0951 - acc: 1.0000     \n",
      "Epoch 809/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0954 - acc: 0.9990     \n",
      "Epoch 810/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0965 - acc: 0.9990     \n",
      "Epoch 811/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0951 - acc: 0.9995     \n",
      "Epoch 812/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0948 - acc: 1.0000     \n",
      "Epoch 813/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0957 - acc: 0.9995     \n",
      "Epoch 814/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0949 - acc: 0.9990     \n",
      "Epoch 815/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0952 - acc: 0.9985     \n",
      "Epoch 816/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0957 - acc: 0.9995     \n",
      "Epoch 817/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0953 - acc: 1.0000     \n",
      "Epoch 818/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0954 - acc: 0.9990     \n",
      "Epoch 819/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0953 - acc: 0.9995     \n",
      "Epoch 820/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0952 - acc: 0.9990     \n",
      "Epoch 821/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0942 - acc: 1.0000     \n",
      "Epoch 822/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0944 - acc: 0.9995     \n",
      "Epoch 823/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0953 - acc: 0.9990     \n",
      "Epoch 824/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0940 - acc: 0.9995     \n",
      "Epoch 825/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0935 - acc: 1.0000     \n",
      "Epoch 826/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0948 - acc: 0.9985     \n",
      "Epoch 827/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0947 - acc: 0.9995     \n",
      "Epoch 828/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0944 - acc: 0.9990     \n",
      "Epoch 829/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0953 - acc: 0.9990     \n",
      "Epoch 830/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0938 - acc: 0.9995     \n",
      "Epoch 831/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0942 - acc: 0.9995     \n",
      "Epoch 832/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0947 - acc: 0.9995     \n",
      "Epoch 833/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0948 - acc: 0.9990     \n",
      "Epoch 834/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0950 - acc: 0.9995     \n",
      "Epoch 835/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0953 - acc: 0.9985     \n",
      "Epoch 836/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0959 - acc: 0.9990     \n",
      "Epoch 837/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0937 - acc: 1.0000     \n",
      "Epoch 838/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0961 - acc: 0.9985     \n",
      "Epoch 839/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0950 - acc: 0.9995     \n",
      "Epoch 840/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0944 - acc: 0.9995     \n",
      "Epoch 841/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0942 - acc: 0.9985     \n",
      "Epoch 842/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.0944 - acc: 0.9995     \n",
      "Epoch 843/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0931 - acc: 1.0000     \n",
      "Epoch 844/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0949 - acc: 0.9985     \n",
      "Epoch 845/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0948 - acc: 0.9990     \n",
      "Epoch 846/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0940 - acc: 0.9990     \n",
      "Epoch 847/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0937 - acc: 1.0000     \n",
      "Epoch 848/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0933 - acc: 0.9995     \n",
      "Epoch 849/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0935 - acc: 0.9995     \n",
      "Epoch 850/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0930 - acc: 0.9995     \n",
      "Epoch 851/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0949 - acc: 0.9985     \n",
      "Epoch 852/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0945 - acc: 0.9990     \n",
      "Epoch 853/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0944 - acc: 0.9985     \n",
      "Epoch 854/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0931 - acc: 0.9995     \n",
      "Epoch 855/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0939 - acc: 0.9995     \n",
      "Epoch 856/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0932 - acc: 0.9995     \n",
      "Epoch 857/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0925 - acc: 0.9995     \n",
      "Epoch 858/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0934 - acc: 1.0000     \n",
      "Epoch 859/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0938 - acc: 0.9990     \n",
      "Epoch 860/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0941 - acc: 1.0000     \n",
      "Epoch 861/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0932 - acc: 0.9990     \n",
      "Epoch 862/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0954 - acc: 0.9975     \n",
      "Epoch 863/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0937 - acc: 1.0000     \n",
      "Epoch 864/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0942 - acc: 0.9995     \n",
      "Epoch 865/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0929 - acc: 1.0000     \n",
      "Epoch 866/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0943 - acc: 0.9995     \n",
      "Epoch 867/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0943 - acc: 0.9990     \n",
      "Epoch 868/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0942 - acc: 0.9990     \n",
      "Epoch 869/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0946 - acc: 0.9995     \n",
      "Epoch 870/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0939 - acc: 0.9995     \n",
      "Epoch 871/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0936 - acc: 0.9995     - ETA: 1s - \n",
      "Epoch 872/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0933 - acc: 0.9995     \n",
      "Epoch 873/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0937 - acc: 0.9985     \n",
      "Epoch 874/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0929 - acc: 0.9995     \n",
      "Epoch 875/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0958 - acc: 0.9980     \n",
      "Epoch 876/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0938 - acc: 0.9990     \n",
      "Epoch 877/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0927 - acc: 1.0000     \n",
      "Epoch 878/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0937 - acc: 1.0000     \n",
      "Epoch 879/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0926 - acc: 1.0000     \n",
      "Epoch 880/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0931 - acc: 0.9995     \n",
      "Epoch 881/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0935 - acc: 0.9990     \n",
      "Epoch 882/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0933 - acc: 1.0000     \n",
      "Epoch 883/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0928 - acc: 0.9990     \n",
      "Epoch 884/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0932 - acc: 0.9995     \n",
      "Epoch 885/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0933 - acc: 0.9990     \n",
      "Epoch 886/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0943 - acc: 0.9995     \n",
      "Epoch 887/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0926 - acc: 1.0000     \n",
      "Epoch 888/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0944 - acc: 0.9985     \n",
      "Epoch 889/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0927 - acc: 0.9995     \n",
      "Epoch 890/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0929 - acc: 1.0000     \n",
      "Epoch 891/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0928 - acc: 0.9995     \n",
      "Epoch 892/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0924 - acc: 1.0000     \n",
      "Epoch 893/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0925 - acc: 1.0000     \n",
      "Epoch 894/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0929 - acc: 0.9990     \n",
      "Epoch 895/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0939 - acc: 0.9985     \n",
      "Epoch 896/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0929 - acc: 0.9995     \n",
      "Epoch 897/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0933 - acc: 0.9995     \n",
      "Epoch 898/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0938 - acc: 0.9975     \n",
      "Epoch 899/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0927 - acc: 0.9995     \n",
      "Epoch 900/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0924 - acc: 1.0000     \n",
      "Epoch 901/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0929 - acc: 0.9990     \n",
      "Epoch 902/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0926 - acc: 0.9990     \n",
      "Epoch 903/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0934 - acc: 0.9975     \n",
      "Epoch 904/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0928 - acc: 0.9990     \n",
      "Epoch 905/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0925 - acc: 0.9995     \n",
      "Epoch 906/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0918 - acc: 1.0000     \n",
      "Epoch 907/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0924 - acc: 0.9995     \n",
      "Epoch 908/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0931 - acc: 0.9980     \n",
      "Epoch 909/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0929 - acc: 0.9995     \n",
      "Epoch 910/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0930 - acc: 0.9990     \n",
      "Epoch 911/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0926 - acc: 0.9990     \n",
      "Epoch 912/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0924 - acc: 0.9995     \n",
      "Epoch 913/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0937 - acc: 0.9995     \n",
      "Epoch 914/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0923 - acc: 0.9990     \n",
      "Epoch 915/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0926 - acc: 0.9990     \n",
      "Epoch 916/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0925 - acc: 0.9995     \n",
      "Epoch 917/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0924 - acc: 0.9990     \n",
      "Epoch 918/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0933 - acc: 0.9990     \n",
      "Epoch 919/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0927 - acc: 0.9990     \n",
      "Epoch 920/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0912 - acc: 1.0000     \n",
      "Epoch 921/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0922 - acc: 0.9995     \n",
      "Epoch 922/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0918 - acc: 1.0000     \n",
      "Epoch 923/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0917 - acc: 0.9995     \n",
      "Epoch 924/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0928 - acc: 0.9995     \n",
      "Epoch 925/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0929 - acc: 0.9995     \n",
      "Epoch 926/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s - loss: 0.0916 - acc: 0.9995     \n",
      "Epoch 927/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0928 - acc: 0.9995     \n",
      "Epoch 928/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0934 - acc: 0.9985     \n",
      "Epoch 929/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0918 - acc: 0.9995     \n",
      "Epoch 930/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0916 - acc: 1.0000     \n",
      "Epoch 931/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0924 - acc: 0.9990     \n",
      "Epoch 932/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0924 - acc: 0.9995     \n",
      "Epoch 933/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0920 - acc: 0.9985     \n",
      "Epoch 934/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0912 - acc: 0.9995     \n",
      "Epoch 935/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0915 - acc: 0.9995     \n",
      "Epoch 936/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0914 - acc: 0.9995     \n",
      "Epoch 937/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0925 - acc: 0.9990     \n",
      "Epoch 938/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0918 - acc: 1.0000     \n",
      "Epoch 939/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0916 - acc: 0.9995     \n",
      "Epoch 940/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0921 - acc: 0.9995     \n",
      "Epoch 941/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0919 - acc: 0.9985     \n",
      "Epoch 942/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0926 - acc: 1.0000     \n",
      "Epoch 943/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0918 - acc: 0.9995     \n",
      "Epoch 944/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0914 - acc: 1.0000     \n",
      "Epoch 945/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0915 - acc: 0.9990     \n",
      "Epoch 946/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0923 - acc: 0.9995     \n",
      "Epoch 947/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0922 - acc: 0.9995     \n",
      "Epoch 948/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0911 - acc: 0.9995     \n",
      "Epoch 949/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0923 - acc: 0.9990     \n",
      "Epoch 950/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0922 - acc: 0.9995     \n",
      "Epoch 951/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0904 - acc: 1.0000     \n",
      "Epoch 952/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0926 - acc: 0.9990     \n",
      "Epoch 953/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0911 - acc: 0.9995     \n",
      "Epoch 954/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0909 - acc: 0.9990     \n",
      "Epoch 955/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0907 - acc: 1.0000     \n",
      "Epoch 956/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0933 - acc: 0.9985     \n",
      "Epoch 957/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0927 - acc: 0.9985     \n",
      "Epoch 958/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0913 - acc: 0.9995     \n",
      "Epoch 959/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0905 - acc: 1.0000     \n",
      "Epoch 960/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0915 - acc: 0.9995     \n",
      "Epoch 961/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0908 - acc: 1.0000     \n",
      "Epoch 962/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0917 - acc: 0.9990     \n",
      "Epoch 963/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0921 - acc: 0.9995     \n",
      "Epoch 964/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0911 - acc: 0.9995     \n",
      "Epoch 965/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0927 - acc: 0.9990     \n",
      "Epoch 966/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0920 - acc: 0.9980     \n",
      "Epoch 967/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0910 - acc: 0.9995     \n",
      "Epoch 968/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0916 - acc: 0.9990     \n",
      "Epoch 969/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0904 - acc: 1.0000     \n",
      "Epoch 970/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0913 - acc: 1.0000     \n",
      "Epoch 971/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0911 - acc: 0.9990     \n",
      "Epoch 972/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0908 - acc: 1.0000     \n",
      "Epoch 973/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0907 - acc: 0.9995     \n",
      "Epoch 974/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0902 - acc: 1.0000     \n",
      "Epoch 975/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0924 - acc: 0.9985     \n",
      "Epoch 976/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0910 - acc: 0.9995     \n",
      "Epoch 977/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0911 - acc: 0.9995     \n",
      "Epoch 978/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0914 - acc: 0.9980     \n",
      "Epoch 979/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0906 - acc: 0.9995     \n",
      "Epoch 980/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0903 - acc: 1.0000     \n",
      "Epoch 981/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0917 - acc: 0.9985     \n",
      "Epoch 982/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0906 - acc: 0.9990     \n",
      "Epoch 983/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0897 - acc: 1.0000     \n",
      "Epoch 984/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0899 - acc: 1.0000     \n",
      "Epoch 985/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0904 - acc: 0.9995     \n",
      "Epoch 986/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0906 - acc: 1.0000     \n",
      "Epoch 987/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0898 - acc: 1.0000     \n",
      "Epoch 988/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0909 - acc: 0.9995     \n",
      "Epoch 989/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0912 - acc: 0.9990     \n",
      "Epoch 990/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0899 - acc: 0.9995     \n",
      "Epoch 991/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0891 - acc: 1.0000     \n",
      "Epoch 992/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0908 - acc: 0.9990     \n",
      "Epoch 993/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0911 - acc: 0.9995     \n",
      "Epoch 994/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0893 - acc: 1.0000     \n",
      "Epoch 995/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0900 - acc: 0.9995     \n",
      "Epoch 996/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0906 - acc: 0.9985     \n",
      "Epoch 997/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0905 - acc: 0.9995     \n",
      "Epoch 998/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0904 - acc: 1.0000     \n",
      "Epoch 999/1000\n",
      "2000/2000 [==============================] - 2s - loss: 0.0908 - acc: 0.9995     \n",
      "Epoch 1000/1000\n",
      "2000/2000 [==============================] - 1s - loss: 0.0894 - acc: 1.0000     \n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "Unable to create file (Unable to truncate a file which is already open)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7f5790e5e7ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Weights/BrainCNNWeights_Visualization.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/amine/.local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amine/anaconda2/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/amine/anaconda2/lib/python2.7/site-packages/h5py/_hl/files.pyc\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1490028130695/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/h5py_1490028130695/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create (/home/ilan/minonda/conda-bld/h5py_1490028130695/work/h5py/h5f.c:2290)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Unable to create file (Unable to truncate a file which is already open)"
     ]
    }
   ],
   "source": [
    "model.fit(X,Y,batch_size=14,nb_epoch=1000,verbose=1,callbacks=[csv_logger])\n",
    "model.save_weights(\"Weights/BrainCNNWeights_Visualization.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "%matplotlib inline\n",
    "heatmap1 = visualize_activation(model,layer_idx=-1, filter_indices=1,seed_input=X[0,:,:,:],input_range = (0.,0.1))[:,:,0]\n",
    "heatmap0 = visualize_activation(model,layer_idx=-1, filter_indices=0,seed_input=X[10,:,:,:],input_range = (0.,0.1))[:,:,0]\n",
    "plotting.plot_connectome(heatmap1,msdl_coords,title=\"reconstructed edges focal injury at node 5\",edge_threshold = '99%')\n",
    "plotting.plot_connectome(heatmap0,msdl_coords,title=\"reconstructed edges focal injury at node 30\",edge_threshold = '99%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
